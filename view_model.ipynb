{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c6ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 16:45:15.151189: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 16:45:45.177519: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "from scipy.optimize import minimize\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "\n",
    "print(\"Number of available GPUs: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# read configuration file\n",
    "with open('config.yml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871e2f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v1_ap99966643.keras',\n",
      " '/nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v2_ap99979441.keras',\n",
      " '/nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v3.keras',\n",
      " '/nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v4.keras',\n",
      " '/nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v5.keras']\n"
     ]
    }
   ],
   "source": [
    "models = sorted(glob(os.path.join(config['data_dir'], 'models', '*.keras')))\n",
    "pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d2e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3d4bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: /nfsdata1/bwedig/lsst-strong-lens-data-challenge/models/v4.keras\n",
      "File size (bytes): 33629702\n",
      "Last modified: 2025-10-11T20:14:06.304979\n",
      "\n",
      "Model name: functional\n",
      "Total params: 2793529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary:\n",
      " Model: \"functional\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
      "│ input_layer         │ (None, 41, 41, 5) │          0 │ -                 │\n",
      "│ (InputLayer)        │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ conv2d (Conv2D)     │ (None, 21, 21,    │     16,128 │ input_layer[0][0] │\n",
      "│                     │ 128)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalization │ (None, 21, 21,    │        512 │ conv2d[0][0]      │\n",
      "│ (BatchNormalizatio… │ 128)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation          │ (None, 21, 21,    │          0 │ batch_normalizat… │\n",
      "│ (Activation)        │ 128)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_1        │ (None, 21, 21,    │          0 │ activation[0][0]  │\n",
      "│ (Activation)        │ 128)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d    │ (None, 21, 21,    │     36,224 │ activation_1[0][… │\n",
      "│ (SeparableConv2D)   │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 21, 21,    │      1,024 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_2        │ (None, 21, 21,    │          0 │ batch_normalizat… │\n",
      "│ (Activation)        │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_1  │ (None, 21, 21,    │     72,192 │ activation_2[0][… │\n",
      "│ (SeparableConv2D)   │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 21, 21,    │      1,024 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ max_pooling2d       │ (None, 11, 11,    │          0 │ batch_normalizat… │\n",
      "│ (MaxPooling2D)      │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ conv2d_1 (Conv2D)   │ (None, 11, 11,    │     33,024 │ activation[0][0]  │\n",
      "│                     │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ add (Add)           │ (None, 11, 11,    │          0 │ max_pooling2d[0]… │\n",
      "│                     │ 256)              │            │ conv2d_1[0][0]    │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_3        │ (None, 11, 11,    │          0 │ add[0][0]         │\n",
      "│ (Activation)        │ 256)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_2  │ (None, 11, 11,    │    137,984 │ activation_3[0][… │\n",
      "│ (SeparableConv2D)   │ 512)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 11, 11,    │      2,048 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │ 512)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_4        │ (None, 11, 11,    │          0 │ batch_normalizat… │\n",
      "│ (Activation)        │ 512)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_3  │ (None, 11, 11,    │    275,456 │ activation_4[0][… │\n",
      "│ (SeparableConv2D)   │ 512)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 11, 11,    │      2,048 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │ 512)              │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ max_pooling2d_1     │ (None, 6, 6, 512) │          0 │ batch_normalizat… │\n",
      "│ (MaxPooling2D)      │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ conv2d_2 (Conv2D)   │ (None, 6, 6, 512) │    131,584 │ add[0][0]         │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ add_1 (Add)         │ (None, 6, 6, 512) │          0 │ max_pooling2d_1[… │\n",
      "│                     │                   │            │ conv2d_2[0][0]    │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_5        │ (None, 6, 6, 512) │          0 │ add_1[0][0]       │\n",
      "│ (Activation)        │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_4  │ (None, 6, 6, 728) │    386,264 │ activation_5[0][… │\n",
      "│ (SeparableConv2D)   │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 6, 6, 728) │      2,912 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_6        │ (None, 6, 6, 728) │          0 │ batch_normalizat… │\n",
      "│ (Activation)        │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_5  │ (None, 6, 6, 728) │    548,912 │ activation_6[0][… │\n",
      "│ (SeparableConv2D)   │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 6, 6, 728) │      2,912 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ max_pooling2d_2     │ (None, 3, 3, 728) │          0 │ batch_normalizat… │\n",
      "│ (MaxPooling2D)      │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ conv2d_3 (Conv2D)   │ (None, 3, 3, 728) │    373,464 │ add_1[0][0]       │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ add_2 (Add)         │ (None, 3, 3, 728) │          0 │ max_pooling2d_2[… │\n",
      "│                     │                   │            │ conv2d_3[0][0]    │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ separable_conv2d_6  │ (None, 3, 3,      │    764,696 │ add_2[0][0]       │\n",
      "│ (SeparableConv2D)   │ 1024)             │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ batch_normalizatio… │ (None, 3, 3,      │      4,096 │ separable_conv2d… │\n",
      "│ (BatchNormalizatio… │ 1024)             │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ activation_7        │ (None, 3, 3,      │          0 │ batch_normalizat… │\n",
      "│ (Activation)        │ 1024)             │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ global_average_poo… │ (None, 1024)      │          0 │ activation_7[0][… │\n",
      "│ (GlobalAveragePool… │                   │            │                   │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ dropout (Dropout)   │ (None, 1024)      │          0 │ global_average_p… │\n",
      "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
      "│ dense (Dense)       │ (None, 1)         │      1,025 │ dropout[0][0]     │\n",
      "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      " Total params: 8,364,013 (31.91 MB)\n",
      " Trainable params: 2,785,241 (10.62 MB)\n",
      " Non-trainable params: 8,288 (32.38 KB)\n",
      " Optimizer params: 5,570,484 (21.25 MB)\n",
      "\n",
      "\n",
      "Model config keys: ['name', 'trainable', 'layers', 'input_layers', 'output_layers']\n",
      "\n",
      "Optimizer type: <class 'keras.src.optimizers.adam.Adam'>\n",
      "Optimizer config: {'name': 'adam', 'learning_rate': 1.0817379916261416e-06, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "\n",
      "Potential sidecar files near model: []\n",
      "\n",
      "No explicit training duration found in nearby files or tensorboard events.\n",
      "You may have to save training history or logs during training (e.g., History object, CSVLogger, or TensorBoard events) to record wall-clock training time.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "import io\n",
    "\n",
    "# cell can use existing variables: model, models, config\n",
    "model_path = models[3]\n",
    "print(\"Model file:\", model_path)\n",
    "print(\"File size (bytes):\", os.path.getsize(model_path))\n",
    "mtime = datetime.fromtimestamp(os.path.getmtime(model_path))\n",
    "print(\"Last modified:\", mtime.isoformat())\n",
    "\n",
    "# basic model info\n",
    "print(\"\\nModel name:\", getattr(model, \"name\", None))\n",
    "try:\n",
    "    print(\"Total params:\", model.count_params())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# capture summary as text\n",
    "try:\n",
    "    buf = io.StringIO()\n",
    "    model.summary(print_fn=lambda s: buf.write(s + \"\\n\"))\n",
    "    summary_text = buf.getvalue()\n",
    "    print(\"\\nModel summary:\\n\", summary_text)\n",
    "except Exception as e:\n",
    "    print(\"\\nCould not get model.summary():\", e)\n",
    "\n",
    "# model config / architecture\n",
    "try:\n",
    "    cfg = model.get_config()\n",
    "    print(\"Model config keys:\", list(cfg.keys()) if isinstance(cfg, dict) else type(cfg))\n",
    "except Exception as e:\n",
    "    print(\"Could not get model.get_config():\", e)\n",
    "\n",
    "# optimizer / compile info\n",
    "opt = getattr(model, \"optimizer\", None)\n",
    "if opt is not None:\n",
    "    try:\n",
    "        print(\"\\nOptimizer type:\", type(opt))\n",
    "        # many optimizers provide get_config()\n",
    "        print(\"Optimizer config:\", opt.get_config())\n",
    "    except Exception as e:\n",
    "        print(\"Could not read optimizer config:\", e)\n",
    "else:\n",
    "    print(\"\\nNo optimizer attached to loaded model.\")\n",
    "\n",
    "# attempt to find saved training history or logs next to the model file\n",
    "base = os.path.splitext(os.path.basename(model_path))[0]\n",
    "d = os.path.dirname(model_path)\n",
    "candidates = []\n",
    "# look for common history/log filenames\n",
    "patterns = [\n",
    "    os.path.join(d, base + \"*.json\"),\n",
    "    os.path.join(d, base + \"*.pkl\"),\n",
    "    os.path.join(d, base + \"*.csv\"),\n",
    "    os.path.join(d, \"*history*.json\"),\n",
    "    os.path.join(d, \"*history*.pkl\"),\n",
    "    os.path.join(d, \"*history*.csv\"),\n",
    "    os.path.join(d, \"*.log\"),\n",
    "]\n",
    "for p in patterns:\n",
    "    candidates.extend(glob(p))\n",
    "candidates = sorted(set(candidates))\n",
    "print(\"\\nPotential sidecar files near model:\", candidates)\n",
    "\n",
    "# helpers to extract duration info from files\n",
    "def seconds_to_str(s):\n",
    "    return str(timedelta(seconds=int(s)))\n",
    "\n",
    "training_durations = []\n",
    "\n",
    "# try JSON / pickle history files\n",
    "for f in candidates:\n",
    "    lower = f.lower()\n",
    "    try:\n",
    "        if lower.endswith(\".json\"):\n",
    "            with open(f, \"r\") as fh:\n",
    "                data = json.load(fh)\n",
    "        elif lower.endswith(\".pkl\"):\n",
    "            with open(f, \"rb\") as fh:\n",
    "                data = pickle.load(fh)\n",
    "        elif lower.endswith(\".csv\"):\n",
    "            import pandas as pd  # already available but safe to import here\n",
    "            df = pd.read_csv(f)\n",
    "            # try to infer timestamps or wall_time\n",
    "            ts_cols = [c for c in df.columns if \"time\" in c.lower() or \"timestamp\" in c.lower() or \"wall\" in c.lower()]\n",
    "            if ts_cols:\n",
    "                times = pd.to_datetime(df[ts_cols[0]], errors=\"coerce\").dropna().astype(int) / 1e9\n",
    "                if len(times) > 1:\n",
    "                    dur = times.max() - times.min()\n",
    "                    training_durations.append((\"csv_timestamps\", f, dur))\n",
    "            # also try to infer if csv is a CSVLogger with 'epoch' rows and optional 'duration' column\n",
    "            dur_cols = [c for c in df.columns if \"duration\" in c.lower() or \"elapsed\" in c.lower()]\n",
    "            if dur_cols:\n",
    "                total = df[dur_cols[0]].astype(float).sum()\n",
    "                training_durations.append((\"csv_duration_col\", f, total))\n",
    "            # store data for inspection\n",
    "            data = {\"csv_rows\": len(df), \"columns\": list(df.columns)}\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # If data looks like a Keras history dict, it often has keys like 'loss' mapping to lists\n",
    "        if isinstance(data, dict):\n",
    "            if all(isinstance(v, list) for v in data.values()):\n",
    "                epochs = len(next(iter(data.values())))\n",
    "                print(f\"\\nFound history-like dict in {f}: epochs ~= {epochs}, keys: {list(data.keys())[:10]}\")\n",
    "                # some users store wall_time or durations in history\n",
    "                for k in (\"time\", \"duration\", \"wall_time\", \"elapsed\"):\n",
    "                    if k in data:\n",
    "                        try:\n",
    "                            dur = sum(data[k]) if isinstance(data[k], list) else float(data[k])\n",
    "                            training_durations.append((\"history_field\", f, dur))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            else:\n",
    "                print(f\"\\nLoaded JSON/Pickle file {f}: keys:\", list(data.keys())[:20])\n",
    "    except Exception as e:\n",
    "        print(\"Could not parse\", f, \":\", e)\n",
    "\n",
    "# try TensorBoard event files in model dir (recursive)\n",
    "tb_events = glob(os.path.join(d, \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
    "if tb_events:\n",
    "    print(\"\\nFound TensorBoard event files:\", tb_events)\n",
    "    for ev in tb_events:\n",
    "        try:\n",
    "            # use tensorflow's summary iterator to read wall_time from events\n",
    "            times = []\n",
    "            for e in tf.compat.v1.train.summary_iterator(ev):\n",
    "                if hasattr(e, \"wall_time\"):\n",
    "                    times.append(e.wall_time)\n",
    "            if times:\n",
    "                dur = max(times) - min(times)\n",
    "                training_durations.append((\"tensorboard_events\", ev, dur))\n",
    "        except Exception as e:\n",
    "            print(\"Could not parse event file\", ev, \":\", e)\n",
    "\n",
    "# if we didn't find explicit durations, try to infer from file timestamps of candidate files\n",
    "if not training_durations and candidates:\n",
    "    mtimes = [os.path.getmtime(f) for f in candidates]\n",
    "    if mtimes:\n",
    "        approx_dur = max(mtimes) - min(mtimes)\n",
    "        training_durations.append((\"file_timestamp_spread\", \"sidecars\", approx_dur))\n",
    "\n",
    "# print discovered durations\n",
    "if training_durations:\n",
    "    print(\"\\nDiscovered training-duration estimates (source, file, seconds):\")\n",
    "    for src, f, sec in training_durations:\n",
    "        print(f\" - {src}: {f} -> {sec:.1f} sec (~{seconds_to_str(sec)})\")\n",
    "else:\n",
    "    print(\"\\nNo explicit training duration found in nearby files or tensorboard events.\")\n",
    "    print(\"You may have to save training history or logs during training (e.g., History object, CSVLogger, or TensorBoard events) to record wall-clock training time.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsst-data-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
