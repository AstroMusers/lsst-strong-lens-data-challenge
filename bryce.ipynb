{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "# from keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "\n",
        "print(\"Number of available GPUs: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# read configuration file\n",
        "with open('config.yml', 'r') as f:\n",
        "    config = yaml.load(f, Loader=yaml.SafeLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = config['data_dir']\n",
        "\n",
        "with np.load(os.path.join(data_dir, 'dataset.npz')) as data:\n",
        "    hsc_lenses = data['hsc_lens']\n",
        "    hsc_nonlenses = data['hsc_nonlens']\n",
        "    slsim_lenses = data['slsim_lens']\n",
        "    slsim_nonlenses = data['slsim_nonlens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(11.4, 12), constrained_layout=True)\n",
        "\n",
        "datasets = [\n",
        "    (slsim_lenses, \"slsim_lenses\"),\n",
        "    (hsc_lenses, \"hsc_lenses\"),\n",
        "    (slsim_nonlenses, \"slsim_nonlenses\"),\n",
        "    (hsc_nonlenses, \"hsc_nonlenses\"),\n",
        "]\n",
        "\n",
        "for ax, (images, title) in zip(axes.flat, datasets):\n",
        "    grid_size = min(25, len(images))\n",
        "    grid_rows = grid_cols = int(np.ceil(np.sqrt(grid_size)))\n",
        "    for i in range(grid_size):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        sub_ax = ax.inset_axes([col/grid_cols, 1-row/grid_rows-1/grid_rows, 1/grid_cols, 1/grid_rows])\n",
        "        sub_ax.imshow(images[i][:,:,:3])\n",
        "        sub_ax.axis(\"off\")  # Hide axes for each image\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")  # Hide main axes\n",
        "\n",
        "plt.suptitle('Dataset Sample')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.concatenate([hsc_lenses, slsim_lenses, hsc_nonlenses, slsim_nonlenses], axis=0)\n",
        "labels = np.array(([1] * (len(hsc_lenses) + len(slsim_lenses))) + ([0] * (len(hsc_nonlenses) + len(slsim_nonlenses))), dtype=np.uint8)\n",
        "print(data.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = tf_data.Dataset.from_tensor_slices((data, labels))\n",
        "ds = ds.shuffle(buffer_size=len(labels), reshuffle_each_iteration=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate split sizes\n",
        "total_size = len(labels)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "print(f'end={total_size}, train={train_size}, val={val_size}, test={test_size}')\n",
        "\n",
        "# Split the dataset\n",
        "train_ds = ds.take(train_size)\n",
        "val_ds = ds.skip(train_size).take(val_size)\n",
        "test_ds = ds.skip(train_size + val_size)\n",
        "\n",
        "print(f\"Train size: {train_ds.cardinality()}, Val size: {val_ds.cardinality()}, Test size: {test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_augmentation_layers = [\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "]\n",
        "\n",
        "def data_augmentation(images):\n",
        "    for layer in data_augmentation_layers:\n",
        "        images = layer(images)\n",
        "    return images\n",
        "\n",
        "# Apply `data_augmentation` to the training images.\n",
        "train_ds = train_ds.map(\n",
        "    lambda img, label: (data_augmentation(img), label),\n",
        "    num_parallel_calls=tf_data.AUTOTUNE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
        "train_ds = train_ds.batch(32).prefetch(tf_data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(32).prefetch(tf_data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(32).prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Train size: {train_ds.cardinality()}, Val size: {val_ds.cardinality()}, Test size: {test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    if num_classes == 2:\n",
        "        units = 1\n",
        "    else:\n",
        "        units = num_classes\n",
        "\n",
        "    outputs = layers.Dense(units, activation=None)(x)\n",
        "    \n",
        "    return models.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def make_model(input_shape, num_classes):\n",
        "#     inputs = keras.Input(shape=input_shape)\n",
        "#     num_bands = input_shape[2]\n",
        "\n",
        "#     # Entry block\n",
        "#     x = layers.Conv2D(128, num_bands, strides=2, padding=\"same\")(inputs)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "#     previous_block_activation = x  # Set aside residual\n",
        "\n",
        "#     for size in [256, 512, 728]:\n",
        "#         x = layers.Activation(\"relu\")(x)\n",
        "#         x = layers.SeparableConv2D(size, num_bands, padding=\"same\")(x)\n",
        "#         x = layers.BatchNormalization()(x)\n",
        "\n",
        "#         x = layers.Activation(\"relu\")(x)\n",
        "#         x = layers.SeparableConv2D(size, num_bands, padding=\"same\")(x)\n",
        "#         x = layers.BatchNormalization()(x)\n",
        "\n",
        "#         x = layers.MaxPooling2D(num_bands, strides=2, padding=\"same\")(x)\n",
        "\n",
        "#         # Project residual\n",
        "#         residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "#             previous_block_activation\n",
        "#         )\n",
        "#         x = layers.add([x, residual])  # Add back residual\n",
        "#         previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "#     x = layers.SeparableConv2D(1024, num_bands, padding=\"same\")(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "#     x = layers.GlobalAveragePooling2D()(x)\n",
        "#     if num_classes == 2:\n",
        "#         units = 1\n",
        "#     else:\n",
        "#         units = num_classes\n",
        "\n",
        "#     x = layers.Dropout(0.25)(x)\n",
        "#     # We specify activation=None so as to return logits\n",
        "#     outputs = layers.Dense(units, activation=None)(x)\n",
        "#     return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# model = make_model(input_shape=(41, 41, 5), num_classes=2)\n",
        "# # keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 25\n",
        "\n",
        "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")  # Or \"/cpu:0\" if no GPU  # MirroredStrategy()\n",
        "with strategy.scope():\n",
        "    model = make_model(input_shape=(41, 41, 5), num_classes=2)\n",
        "    # keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(3e-4),\n",
        "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        # keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=val_ds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "accuracy = history.history['acc']\n",
        "val_accuracy = history.history['val_acc']\n",
        "\n",
        "_, ax = plt.subplots(1, 2, figsize=(7, 3), constrained_layout=True)\n",
        "epoch_list = range(1, len(loss) + 1)\n",
        "\n",
        "ax[0].plot(epoch_list, loss, 'bo-', label='Training')\n",
        "ax[0].plot(epoch_list, val_loss, 'ro-', label='Validation')\n",
        "ax[0].set_title('Loss')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "# ax[0].set_ylim(0, 1)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(epoch_list, accuracy, 'bo-', label='Training')\n",
        "ax[1].plot(epoch_list, val_accuracy, 'ro-', label='Validation')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "# ax[1].set_ylim(0, 1)\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = model.evaluate(test_ds, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "for idx, (images_batch, labels_batch) in enumerate(test_ds.take(1)):\n",
        "    preds = model.predict(images_batch)\n",
        "    probs = keras.ops.sigmoid(preds).numpy().flatten()\n",
        "    for i in range(min(16, images_batch.shape[0])):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images_batch[i].numpy()[:,:,:3])\n",
        "        plt.title(f\"Pred: {probs[i]:.2f}\\nLabel: {int(labels_batch[i])}\")\n",
        "        plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect predictions and true labels from test_ds\n",
        "y_true = []\n",
        "y_scores = []\n",
        "\n",
        "for images_batch, labels_batch in test_ds:\n",
        "    preds = model.predict(images_batch, verbose=0)\n",
        "    probs = keras.ops.sigmoid(preds).numpy().flatten()  # Use sigmoid to get probabilities\n",
        "    y_scores.extend(probs)\n",
        "    y_true.extend(labels_batch.numpy().flatten())\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "ap_score = average_precision_score(y_true, y_scores)\n",
        "\n",
        "plt.figure(figsize=(3.5, 3))\n",
        "plt.plot(recall, precision, label=f'AP={ap_score:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect misclassified images\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "misclassified_probs = []\n",
        "\n",
        "for images_batch, labels_batch in test_ds:\n",
        "    preds = model.predict(images_batch, verbose=0)\n",
        "    probs = keras.ops.sigmoid(preds).numpy().flatten()\n",
        "    \n",
        "    # Convert probabilities to binary predictions (threshold at 0.5)\n",
        "    pred_labels = (probs > 0.5).astype(int)\n",
        "    true_labels = labels_batch.numpy().flatten().astype(int)\n",
        "    \n",
        "    # Find misclassified samples\n",
        "    misclassified_mask = pred_labels != true_labels\n",
        "    \n",
        "    if np.any(misclassified_mask):\n",
        "        misclassified_images.extend(images_batch[misclassified_mask].numpy())\n",
        "        misclassified_labels.extend(true_labels[misclassified_mask])\n",
        "        misclassified_probs.extend(probs[misclassified_mask])\n",
        "\n",
        "# Display misclassified images\n",
        "num_to_show = min(16, len(misclassified_images))\n",
        "if num_to_show > 0:\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(num_to_show):\n",
        "        ax = plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(misclassified_images[i][:,:,:3])\n",
        "        true_label = misclassified_labels[i]\n",
        "        pred_prob = misclassified_probs[i]\n",
        "        pred_label = 1 if pred_prob > 0.5 else 0\n",
        "        plt.title(f\"True: {true_label}, Pred: {pred_label}\\nProb: {pred_prob:.3f}\", fontsize=10)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Misclassified Images (Total: {len(misclassified_images)})\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Total misclassified images: {len(misclassified_images)}\")\n",
        "else:\n",
        "    print(\"No misclassified images found!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lsst-data-challenge",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
